# ChatGLM2-6B

<p align="center">
ğŸ¤— <a href="https://huggingface.co/THUDM/chatglm2-6b" target="_blank">HF Repo</a> â€¢ ğŸ¦ <a href="https://twitter.com/thukeg" target="_blank">Twitter</a> â€¢ ğŸ“ƒ <a href="https://arxiv.org/abs/2103.10360" target="_blank">[GLM@ACL 22]</a> <a href="https://github.com/THUDM/GLM" target="_blank">[GitHub]</a> â€¢ ğŸ“ƒ <a href="https://arxiv.org/abs/2210.02414" target="_blank">[GLM-130B@ICLR 23]</a> <a href="https://github.com/THUDM/GLM-130B" target="_blank">[GitHub]</a> <br>
</p>
<p align="center">
    ğŸ‘‹ åŠ å…¥æˆ‘ä»¬çš„ <a href="https://join.slack.com/t/chatglm/shared_invite/zt-1udqapmrr-ocT1DS_mxWe6dDY8ahRWzg" target="_blank">Slack</a> å’Œ <a href="resources/WECHAT.md" target="_blank">WeChat</a>
</p>

*Read this in [English](README_EN.md)*

## ä»‹ç»

ChatGLM**2**-6B æ˜¯å¼€æºä¸­è‹±åŒè¯­å¯¹è¯æ¨¡å‹ [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B) çš„ç¬¬äºŒä»£ç‰ˆæœ¬ï¼Œåœ¨ä¿ç•™äº†åˆä»£æ¨¡å‹å¯¹è¯æµç•…ã€éƒ¨ç½²é—¨æ§›è¾ƒä½ç­‰ä¼—å¤šä¼˜ç§€ç‰¹æ€§çš„åŸºç¡€ä¹‹ä¸Šï¼ŒChatGLM**2**-6B å¼•å…¥äº†å¦‚ä¸‹æ–°ç‰¹æ€§ï¼š

1. **æ›´å¼ºå¤§çš„æ€§èƒ½**ï¼šåŸºäº ChatGLM åˆä»£æ¨¡å‹çš„å¼€å‘ç»éªŒï¼Œæˆ‘ä»¬å…¨é¢å‡çº§äº† ChatGLM2-6B çš„åŸºåº§æ¨¡å‹ã€‚ChatGLM2-6B ä½¿ç”¨äº† [GLM](https://github.com/THUDM/GLM) çš„æ··åˆç›®æ ‡å‡½æ•°ï¼Œç»è¿‡äº† 1.4T ä¸­è‹±æ ‡è¯†ç¬¦çš„é¢„è®­ç»ƒä¸äººç±»åå¥½å¯¹é½è®­ç»ƒï¼Œ[è¯„æµ‹ç»“æœ](#è¯„æµ‹ç»“æœ)æ˜¾ç¤ºï¼Œç›¸æ¯”äºåˆä»£æ¨¡å‹ï¼ŒChatGLM2-6B åœ¨ MMLUï¼ˆ+23%ï¼‰ã€CEvalï¼ˆ+33%ï¼‰ã€GSM8Kï¼ˆ+571%ï¼‰ ã€BBHï¼ˆ+60%ï¼‰ç­‰æ•°æ®é›†ä¸Šçš„æ€§èƒ½å–å¾—äº†å¤§å¹…åº¦çš„æå‡ï¼Œåœ¨åŒå°ºå¯¸å¼€æºæ¨¡å‹ä¸­å…·æœ‰è¾ƒå¼ºçš„ç«äº‰åŠ›ã€‚
2. **æ›´é•¿çš„ä¸Šä¸‹æ–‡**ï¼šåŸºäº [FlashAttention](https://github.com/HazyResearch/flash-attention) æŠ€æœ¯ï¼Œæˆ‘ä»¬å°†åŸºåº§æ¨¡å‹çš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆContext Lengthï¼‰ç”± ChatGLM-6B çš„ 2K æ‰©å±•åˆ°äº† 32Kï¼Œå¹¶åœ¨å¯¹è¯é˜¶æ®µä½¿ç”¨ 8K çš„ä¸Šä¸‹æ–‡é•¿åº¦è®­ç»ƒï¼Œå…è®¸æ›´å¤šè½®æ¬¡çš„å¯¹è¯ã€‚ä½†å½“å‰ç‰ˆæœ¬çš„ ChatGLM2-6B å¯¹å•è½®è¶…é•¿æ–‡æ¡£çš„ç†è§£èƒ½åŠ›æœ‰é™ï¼Œæˆ‘ä»¬ä¼šåœ¨åç»­è¿­ä»£å‡çº§ä¸­ç€é‡è¿›è¡Œä¼˜åŒ–ã€‚
3. **æ›´é«˜æ•ˆçš„æ¨ç†**ï¼šåŸºäº [Multi-Query Attention](http://arxiv.org/abs/1911.02150) æŠ€æœ¯ï¼ŒChatGLM2-6B æœ‰æ›´é«˜æ•ˆçš„æ¨ç†é€Ÿåº¦å’Œæ›´ä½çš„æ˜¾å­˜å ç”¨ï¼šåœ¨å®˜æ–¹çš„æ¨¡å‹å®ç°ä¸‹ï¼Œæ¨ç†é€Ÿåº¦ç›¸æ¯”åˆä»£æå‡äº† 42%ï¼ŒINT4 é‡åŒ–ä¸‹ï¼Œ6G æ˜¾å­˜æ”¯æŒçš„å¯¹è¯é•¿åº¦ç”± 1K æå‡åˆ°äº† 8Kã€‚
4. **æ›´å¼€æ”¾çš„åè®®**ï¼šChatGLM2-6B æƒé‡å¯¹å­¦æœ¯ç ”ç©¶**å®Œå…¨å¼€æ”¾**ï¼Œåœ¨è·å¾—å®˜æ–¹çš„ä¹¦é¢è®¸å¯åï¼Œäº¦**å…è®¸å•†ä¸šä½¿ç”¨**ã€‚å¦‚æœæ‚¨å‘ç°æˆ‘ä»¬çš„å¼€æºæ¨¡å‹å¯¹æ‚¨çš„ä¸šåŠ¡æœ‰ç”¨ï¼Œæˆ‘ä»¬æ¬¢è¿æ‚¨å¯¹ä¸‹ä¸€ä»£æ¨¡å‹ ChatGLM3 ç ”å‘çš„æèµ ã€‚

-----

ChatGLM2-6B å¼€æºæ¨¡å‹æ—¨åœ¨ä¸å¼€æºç¤¾åŒºä¸€èµ·æ¨åŠ¨å¤§æ¨¡å‹æŠ€æœ¯å‘å±•ï¼Œæ³è¯·å¼€å‘è€…å’Œå¤§å®¶éµå®ˆ[å¼€æºåè®®](MODEL_LICENSE)ï¼Œå‹¿å°†å¼€æºæ¨¡å‹å’Œä»£ç åŠåŸºäºå¼€æºé¡¹ç›®äº§ç”Ÿçš„è¡ç”Ÿç‰©ç”¨äºä»»ä½•å¯èƒ½ç»™å›½å®¶å’Œç¤¾ä¼šå¸¦æ¥å±å®³çš„ç”¨é€”ä»¥åŠç”¨äºä»»ä½•æœªç»è¿‡å®‰å…¨è¯„ä¼°å’Œå¤‡æ¡ˆçš„æœåŠ¡ã€‚**ç›®å‰ï¼Œæœ¬é¡¹ç›®å›¢é˜ŸæœªåŸºäº ChatGLM2-6B å¼€å‘ä»»ä½•åº”ç”¨ï¼ŒåŒ…æ‹¬ç½‘é¡µç«¯ã€å®‰å“ã€è‹¹æœ iOS åŠ Windows App ç­‰åº”ç”¨ã€‚**

å°½ç®¡æ¨¡å‹åœ¨è®­ç»ƒçš„å„ä¸ªé˜¶æ®µéƒ½å°½åŠ›ç¡®ä¿æ•°æ®çš„åˆè§„æ€§å’Œå‡†ç¡®æ€§ï¼Œä½†ç”±äº ChatGLM2-6B æ¨¡å‹è§„æ¨¡è¾ƒå°ï¼Œä¸”æ¨¡å‹å—æ¦‚ç‡éšæœºæ€§å› ç´ å½±å“ï¼Œæ— æ³•ä¿è¯è¾“å‡ºå†…å®¹çš„å‡†ç¡®æ€§ï¼Œä¸”æ¨¡å‹æ˜“è¢«è¯¯å¯¼ã€‚**æœ¬é¡¹ç›®ä¸æ‰¿æ‹…å¼€æºæ¨¡å‹å’Œä»£ç å¯¼è‡´çš„æ•°æ®å®‰å…¨ã€èˆ†æƒ…é£é™©æˆ–å‘ç”Ÿä»»ä½•æ¨¡å‹è¢«è¯¯å¯¼ã€æ»¥ç”¨ã€ä¼ æ’­ã€ä¸å½“åˆ©ç”¨è€Œäº§ç”Ÿçš„é£é™©å’Œè´£ä»»ã€‚**

## è¯„æµ‹ç»“æœ
æˆ‘ä»¬é€‰å–äº†éƒ¨åˆ†ä¸­è‹±æ–‡å…¸å‹æ•°æ®é›†è¿›è¡Œäº†è¯„æµ‹ï¼Œä»¥ä¸‹ä¸º ChatGLM2-6B æ¨¡å‹åœ¨ [MMLU](https://github.com/hendrycks/test) (è‹±æ–‡)ã€[C-Eval](https://cevalbenchmark.com/static/leaderboard.html)ï¼ˆä¸­æ–‡ï¼‰ã€[GSM8K](https://github.com/openai/grade-school-math)ï¼ˆæ•°å­¦ï¼‰ã€[BBH](https://github.com/suzgunmirac/BIG-Bench-Hard)ï¼ˆè‹±æ–‡ï¼‰ ä¸Šçš„æµ‹è¯„ç»“æœã€‚åœ¨ [evaluation](./evaluation/README.md) ä¸­æä¾›äº†åœ¨ C-Eval ä¸Šè¿›è¡Œæµ‹è¯„çš„è„šæœ¬ã€‚

### MMLU

| Model | Average | STEM | Social Sciences | Humanities | Others |
| ----- | ----- | ---- | ----- | ----- | ----- |
| ChatGLM-6B | 40.63 | 33.89 | 44.84 | 39.02 | 45.71 |
| ChatGLM2-6B (base) | 47.86 | 41.20 | 54.44 | 43.66 | 54.46 |
| ChatGLM2-6B | 45.46 | 40.06 | 51.61 | 41.23 | 51.24 |

> Chat æ¨¡å‹ä½¿ç”¨ zero-shot CoT (Chain-of-Thought) çš„æ–¹æ³•æµ‹è¯•ï¼ŒBase æ¨¡å‹ä½¿ç”¨ few-shot answer-only çš„æ–¹æ³•æµ‹è¯•

### C-Eval

| Model | Average | STEM | Social Sciences | Humanities | Others |
| ----- | ---- | ---- | ----- | ----- | ----- |
| ChatGLM-6B | 38.9 | 33.3 | 48.3 | 41.3 | 38.0 |
| ChatGLM2-6B (base) | 51.7 | 48.6 | 60.5 | 51.3 | 49.8 |
| ChatGLM2-6B | 50.1 | 46.4	| 60.4 | 50.6 | 46.9 | 

> Chat æ¨¡å‹ä½¿ç”¨ zero-shot CoT çš„æ–¹æ³•æµ‹è¯•ï¼ŒBase æ¨¡å‹ä½¿ç”¨ few-shot answer only çš„æ–¹æ³•æµ‹è¯•

### GSM8K

| Model | Accuracy | Accuracy (Chinese)* |
| ----- | ----- | ----- |
| ChatGLM-6B | 4.82 | 5.85 |
| ChatGLM2-6B (base) | 32.37 | 28.95 |
| ChatGLM2-6B | 28.05 | 20.45 |

> æ‰€æœ‰æ¨¡å‹å‡ä½¿ç”¨ few-shot CoT çš„æ–¹æ³•æµ‹è¯•ï¼ŒCoT prompt æ¥è‡ª http://arxiv.org/abs/2201.11903
> 
> \* æˆ‘ä»¬ä½¿ç”¨ç¿»è¯‘ API ç¿»è¯‘äº† GSM8K ä¸­çš„ 500 é“é¢˜ç›®å’Œ CoT prompt å¹¶è¿›è¡Œäº†äººå·¥æ ¡å¯¹


### BBH

| Model | Accuracy |
| ----- | ----- |
| ChatGLM-6B | 18.73 |
| ChatGLM2-6B (base) | 33.68 |
| ChatGLM2-6B | 30.00 |

> æ‰€æœ‰æ¨¡å‹å‡ä½¿ç”¨ few-shot CoT çš„æ–¹æ³•æµ‹è¯•ï¼ŒCoT prompt æ¥è‡ª https://github.com/suzgunmirac/BIG-Bench-Hard/tree/main/cot-prompts

## æ¨ç†æ€§èƒ½
ChatGLM2-6B ä½¿ç”¨äº† [Multi-Query Attention](http://arxiv.org/abs/1911.02150)ï¼Œæé«˜äº†ç”Ÿæˆé€Ÿåº¦ã€‚ç”Ÿæˆ 2000 ä¸ªå­—ç¬¦çš„å¹³å‡é€Ÿåº¦å¯¹æ¯”å¦‚ä¸‹

| Model | æ¨ç†é€Ÿåº¦ (å­—ç¬¦/ç§’) |
| ----  | -----  |
| ChatGLM-6B  | 31.49 |
| ChatGLM2-6B | 44.62 |

> ä½¿ç”¨å®˜æ–¹å®ç°ï¼Œbatch size = 1ï¼Œmax length = 2048ï¼Œbf16 ç²¾åº¦ï¼Œæµ‹è¯•ç¡¬ä»¶ä¸º A100-SXM4-80Gï¼Œè½¯ä»¶ç¯å¢ƒä¸º PyTorch 2.0.1

Multi-Query Attention åŒæ—¶ä¹Ÿé™ä½äº†ç”Ÿæˆè¿‡ç¨‹ä¸­ KV Cache çš„æ˜¾å­˜å ç”¨ï¼Œæ­¤å¤–ï¼ŒChatGLM2-6B é‡‡ç”¨ Causal Mask è¿›è¡Œå¯¹è¯è®­ç»ƒï¼Œè¿ç»­å¯¹è¯æ—¶å¯å¤ç”¨å‰é¢è½®æ¬¡çš„ KV Cacheï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–äº†æ˜¾å­˜å ç”¨ã€‚å› æ­¤ï¼Œä½¿ç”¨ 6GB æ˜¾å­˜çš„æ˜¾å¡è¿›è¡Œ INT4 é‡åŒ–çš„æ¨ç†æ—¶ï¼Œåˆä»£çš„ ChatGLM-6B æ¨¡å‹æœ€å¤šèƒ½å¤Ÿç”Ÿæˆ 1119 ä¸ªå­—ç¬¦å°±ä¼šæç¤ºæ˜¾å­˜è€—å°½ï¼Œè€Œ ChatGLM2-6B èƒ½å¤Ÿç”Ÿæˆè‡³å°‘ 8192 ä¸ªå­—ç¬¦ã€‚

| **é‡åŒ–ç­‰çº§** | **ç¼–ç  2048 é•¿åº¦çš„æœ€å°æ˜¾å­˜** | **ç”Ÿæˆ 8192 é•¿åº¦çš„æœ€å°æ˜¾å­˜** |
| -------------- |---------------------|---------------------|
| FP16 / BF16 | 13.1 GB             | 12.8 GB             | 
| INT8           | 8.2 GB              | 8.1 GB              |
| INT4           | 5.5 GB              | 5.1 GB              |

> ChatGLM2-6B åˆ©ç”¨äº† PyTorch 2.0 å¼•å…¥çš„ `torch.nn.functional.scaled_dot_product_attention` å®ç°é«˜æ•ˆçš„ Attention è®¡ç®—ï¼Œå¦‚æœ PyTorch ç‰ˆæœ¬è¾ƒä½åˆ™ä¼š fallback åˆ°æœ´ç´ çš„ Attention å®ç°ï¼Œå‡ºç°æ˜¾å­˜å ç”¨é«˜äºä¸Šè¡¨çš„æƒ…å†µã€‚

æˆ‘ä»¬ä¹Ÿæµ‹è¯•äº†é‡åŒ–å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ã€‚ç»“æœè¡¨æ˜ï¼Œé‡åŒ–å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“åœ¨å¯æ¥å—èŒƒå›´å†…ã€‚

| é‡åŒ–ç­‰çº§ | Accuracy (MMLU) | Accuracy (C-Eval dev) |
| ----- | ----- |-----------------------|
| BF16 | 45.47 | 53.57                 |
| INT4 | 43.13 | 50.30                 |



## ChatGLM2-6B ç¤ºä¾‹

ç›¸æ¯”äºåˆä»£æ¨¡å‹ï¼ŒChatGLM2-6B å¤šä¸ªç»´åº¦çš„èƒ½åŠ›éƒ½å–å¾—äº†æå‡ï¼Œä»¥ä¸‹æ˜¯ä¸€äº›å¯¹æ¯”ç¤ºä¾‹ã€‚æ›´å¤š ChatGLM2-6B çš„å¯èƒ½ï¼Œç­‰å¾…ä½ æ¥æ¢ç´¢å‘ç°ï¼

<details><summary><b>æ•°ç†é€»è¾‘</b></summary>

![](resources/math.png)

</details>

<details><summary><b>çŸ¥è¯†æ¨ç†</b></summary>

![](resources/knowledge.png)

</details>

<details><summary><b>é•¿æ–‡æ¡£ç†è§£</b></summary>

![](resources/long-context.png)

</details>

## ä½¿ç”¨æ–¹å¼
### ç¯å¢ƒå®‰è£…
é¦–å…ˆéœ€è¦ä¸‹è½½æœ¬ä»“åº“ï¼š
```shell
git clone https://github.com/THUDM/ChatGLM2-6B
cd ChatGLM2-6B
```

ç„¶åä½¿ç”¨ pip å®‰è£…ä¾èµ–ï¼š`pip install -r requirements.txt`ï¼Œå…¶ä¸­ `transformers` åº“ç‰ˆæœ¬æ¨èä¸º `4.30.2`ï¼Œ`torch` æ¨èä½¿ç”¨ 2.0 ä»¥ä¸Šçš„ç‰ˆæœ¬ï¼Œä»¥è·å¾—æœ€ä½³çš„æ¨ç†æ€§èƒ½ã€‚

### ä»£ç è°ƒç”¨ 

å¯ä»¥é€šè¿‡å¦‚ä¸‹ä»£ç è°ƒç”¨ ChatGLM2-6B æ¨¡å‹æ¥ç”Ÿæˆå¯¹è¯ï¼š

```python
>>> from transformers import AutoTokenizer, AutoModel
>>> tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm2-6b", trust_remote_code=True)
>>> model = AutoModel.from_pretrained("THUDM/chatglm2-6b", trust_remote_code=True, device='cuda')
>>> model = model.eval()
>>> response, history = model.chat(tokenizer, "ä½ å¥½", history=[])
>>> print(response)
ä½ å¥½ğŸ‘‹!æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM2-6B,å¾ˆé«˜å…´è§åˆ°ä½ ,æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚
>>> response, history = model.chat(tokenizer, "æ™šä¸Šç¡ä¸ç€åº”è¯¥æ€ä¹ˆåŠ", history=history)
>>> print(response)
æ™šä¸Šç¡ä¸ç€å¯èƒ½ä¼šè®©ä½ æ„Ÿåˆ°ç„¦è™‘æˆ–ä¸èˆ’æœ,ä½†ä»¥ä¸‹æ˜¯ä¸€äº›å¯ä»¥å¸®åŠ©ä½ å…¥ç¡çš„æ–¹æ³•:

1. åˆ¶å®šè§„å¾‹çš„ç¡çœ æ—¶é—´è¡¨:ä¿æŒè§„å¾‹çš„ç¡çœ æ—¶é—´è¡¨å¯ä»¥å¸®åŠ©ä½ å»ºç«‹å¥åº·çš„ç¡çœ ä¹ æƒ¯,ä½¿ä½ æ›´å®¹æ˜“å…¥ç¡ã€‚å°½é‡åœ¨æ¯å¤©çš„ç›¸åŒæ—¶é—´ä¸ŠåºŠ,å¹¶åœ¨åŒä¸€æ—¶é—´èµ·åºŠã€‚
2. åˆ›é€ ä¸€ä¸ªèˆ’é€‚çš„ç¡çœ ç¯å¢ƒ:ç¡®ä¿ç¡çœ ç¯å¢ƒèˆ’é€‚,å®‰é™,é»‘æš—ä¸”æ¸©åº¦é€‚å®œã€‚å¯ä»¥ä½¿ç”¨èˆ’é€‚çš„åºŠä¸Šç”¨å“,å¹¶ä¿æŒæˆ¿é—´é€šé£ã€‚
3. æ”¾æ¾èº«å¿ƒ:åœ¨ç¡å‰åšäº›æ”¾æ¾çš„æ´»åŠ¨,ä¾‹å¦‚æ³¡ä¸ªçƒ­æ°´æ¾¡,å¬äº›è½»æŸ”çš„éŸ³ä¹,é˜…è¯»ä¸€äº›æœ‰è¶£çš„ä¹¦ç±ç­‰,æœ‰åŠ©äºç¼“è§£ç´§å¼ å’Œç„¦è™‘,ä½¿ä½ æ›´å®¹æ˜“å…¥ç¡ã€‚
4. é¿å…é¥®ç”¨å«æœ‰å’–å•¡å› çš„é¥®æ–™:å’–å•¡å› æ˜¯ä¸€ç§åˆºæ¿€æ€§ç‰©è´¨,ä¼šå½±å“ä½ çš„ç¡çœ è´¨é‡ã€‚å°½é‡é¿å…åœ¨ç¡å‰é¥®ç”¨å«æœ‰å’–å•¡å› çš„é¥®æ–™,ä¾‹å¦‚å’–å•¡,èŒ¶å’Œå¯ä¹ã€‚
5. é¿å…åœ¨åºŠä¸Šåšä¸ç¡çœ æ— å…³çš„äº‹æƒ…:åœ¨åºŠä¸Šåšäº›ä¸ç¡çœ æ— å…³çš„äº‹æƒ…,ä¾‹å¦‚çœ‹ç”µå½±,ç©æ¸¸æˆæˆ–å·¥ä½œç­‰,å¯èƒ½ä¼šå¹²æ‰°ä½ çš„ç¡çœ ã€‚
6. å°è¯•å‘¼å¸æŠ€å·§:æ·±å‘¼å¸æ˜¯ä¸€ç§æ”¾æ¾æŠ€å·§,å¯ä»¥å¸®åŠ©ä½ ç¼“è§£ç´§å¼ å’Œç„¦è™‘,ä½¿ä½ æ›´å®¹æ˜“å…¥ç¡ã€‚è¯•ç€æ…¢æ…¢å¸æ°”,ä¿æŒå‡ ç§’é’Ÿ,ç„¶åç¼“æ…¢å‘¼æ°”ã€‚

å¦‚æœè¿™äº›æ–¹æ³•æ— æ³•å¸®åŠ©ä½ å…¥ç¡,ä½ å¯ä»¥è€ƒè™‘å’¨è¯¢åŒ»ç”Ÿæˆ–ç¡çœ ä¸“å®¶,å¯»æ±‚è¿›ä¸€æ­¥çš„å»ºè®®ã€‚
```

#### ä»æœ¬åœ°åŠ è½½æ¨¡å‹
ä»¥ä¸Šä»£ç ä¼šç”± `transformers` è‡ªåŠ¨ä¸‹è½½æ¨¡å‹å®ç°å’Œå‚æ•°ã€‚å®Œæ•´çš„æ¨¡å‹å®ç°åœ¨ [Hugging Face Hub](https://huggingface.co/THUDM/chatglm2-6b)ã€‚å¦‚æœä½ çš„ç½‘ç»œç¯å¢ƒè¾ƒå·®ï¼Œä¸‹è½½æ¨¡å‹å‚æ•°å¯èƒ½ä¼šèŠ±è´¹è¾ƒé•¿æ—¶é—´ç”šè‡³å¤±è´¥ã€‚æ­¤æ—¶å¯ä»¥å…ˆå°†æ¨¡å‹ä¸‹è½½åˆ°æœ¬åœ°ï¼Œç„¶åä»æœ¬åœ°åŠ è½½ã€‚

ä» Hugging Face Hub ä¸‹è½½æ¨¡å‹éœ€è¦å…ˆ[å®‰è£…Git LFS](https://docs.github.com/zh/repositories/working-with-files/managing-large-files/installing-git-large-file-storage)ï¼Œç„¶åè¿è¡Œ
```Shell
git clone https://huggingface.co/THUDM/chatglm2-6b
```

å¦‚æœä½ ä» Hugging Face Hub ä¸Šä¸‹è½½ checkpoint çš„é€Ÿåº¦è¾ƒæ…¢ï¼Œå¯ä»¥åªä¸‹è½½æ¨¡å‹å®ç°
```Shell
GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/THUDM/chatglm2-6b
```
ç„¶åä»[è¿™é‡Œ](https://cloud.tsinghua.edu.cn/d/674208019e314311ab5c/)æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹å‚æ•°æ–‡ä»¶ï¼Œå¹¶å°†ä¸‹è½½çš„æ–‡ä»¶æ›¿æ¢åˆ°æœ¬åœ°çš„ `chatglm2-6b` ç›®å½•ä¸‹ã€‚


å°†æ¨¡å‹ä¸‹è½½åˆ°æœ¬åœ°ä¹‹åï¼Œå°†ä»¥ä¸Šä»£ç ä¸­çš„ `THUDM/chatglm2-6b` æ›¿æ¢ä¸ºä½ æœ¬åœ°çš„ `chatglm2-6b` æ–‡ä»¶å¤¹çš„è·¯å¾„ï¼Œå³å¯ä»æœ¬åœ°åŠ è½½æ¨¡å‹ã€‚

æ¨¡å‹çš„å®ç°ä»ç„¶å¤„åœ¨å˜åŠ¨ä¸­ã€‚å¦‚æœå¸Œæœ›å›ºå®šä½¿ç”¨çš„æ¨¡å‹å®ç°ä»¥ä¿è¯å…¼å®¹æ€§ï¼Œå¯ä»¥åœ¨ `from_pretrained` çš„è°ƒç”¨ä¸­å¢åŠ  `revision="v1.0"` å‚æ•°ã€‚`v1.0` æ˜¯å½“å‰æœ€æ–°çš„ç‰ˆæœ¬å·ï¼Œå®Œæ•´çš„ç‰ˆæœ¬åˆ—è¡¨å‚è§ [Change Log](https://huggingface.co/THUDM/chatglm2-6b#change-log)ã€‚

### ç½‘é¡µç‰ˆ Demo

![web-demo](resources/web-demo.gif)

é¦–å…ˆå®‰è£… Gradioï¼š`pip install gradio`ï¼Œç„¶åè¿è¡Œä»“åº“ä¸­çš„ [web_demo.py](web_demo.py)ï¼š 

```shell
python web_demo.py
```

ç¨‹åºä¼šè¿è¡Œä¸€ä¸ª Web Serverï¼Œå¹¶è¾“å‡ºåœ°å€ã€‚åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€è¾“å‡ºçš„åœ°å€å³å¯ä½¿ç”¨ã€‚
> é»˜è®¤ä½¿ç”¨äº† `share=False` å¯åŠ¨ï¼Œä¸ä¼šç”Ÿæˆå…¬ç½‘é“¾æ¥ã€‚å¦‚æœ‰éœ€è¦å…¬ç½‘è®¿é—®çš„éœ€æ±‚ï¼Œå¯ä»¥ä¿®æ”¹ä¸º `share=True` å¯åŠ¨ã€‚
> 

æ„Ÿè°¢ [@AdamBear](https://github.com/AdamBear) å®ç°äº†åŸºäº Streamlit çš„ç½‘é¡µç‰ˆ Demo `web_demo2.py`ã€‚ä½¿ç”¨æ—¶é¦–å…ˆéœ€è¦é¢å¤–å®‰è£…ä»¥ä¸‹ä¾èµ–ï¼š
```shell
pip install streamlit streamlit-chat
```
ç„¶åé€šè¿‡ä»¥ä¸‹å‘½ä»¤è¿è¡Œï¼š
```shell
streamlit run web_demo2.py
```
ç»æµ‹è¯•ï¼Œå¦‚æœè¾“å…¥çš„ prompt è¾ƒé•¿çš„è¯ï¼Œä½¿ç”¨åŸºäº Streamlit çš„ç½‘é¡µç‰ˆ Demo ä¼šæ›´æµç•…ã€‚

### å‘½ä»¤è¡Œ Demo

![cli-demo](resources/cli-demo.png)

è¿è¡Œä»“åº“ä¸­ [cli_demo.py](cli_demo.py)ï¼š

```shell
python cli_demo.py
```

ç¨‹åºä¼šåœ¨å‘½ä»¤è¡Œä¸­è¿›è¡Œäº¤äº’å¼çš„å¯¹è¯ï¼Œåœ¨å‘½ä»¤è¡Œä¸­è¾“å…¥æŒ‡ç¤ºå¹¶å›è½¦å³å¯ç”Ÿæˆå›å¤ï¼Œè¾“å…¥ `clear` å¯ä»¥æ¸…ç©ºå¯¹è¯å†å²ï¼Œè¾“å…¥ `stop` ç»ˆæ­¢ç¨‹åºã€‚

### API éƒ¨ç½²
é¦–å…ˆéœ€è¦å®‰è£…é¢å¤–çš„ä¾èµ– `pip install fastapi uvicorn`ï¼Œç„¶åè¿è¡Œä»“åº“ä¸­çš„ [api.py](api.py)ï¼š
```shell
python api.py
```
é»˜è®¤éƒ¨ç½²åœ¨æœ¬åœ°çš„ 8000 ç«¯å£ï¼Œé€šè¿‡ POST æ–¹æ³•è¿›è¡Œè°ƒç”¨
```shell
curl -X POST "http://127.0.0.1:8000" \
     -H 'Content-Type: application/json' \
     -d '{"prompt": "ä½ å¥½", "history": []}'
```
å¾—åˆ°çš„è¿”å›å€¼ä¸º
```shell
{
  "response":"ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM2-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚",
  "history":[["ä½ å¥½","ä½ å¥½ğŸ‘‹ï¼æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM2-6Bï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚"]],
  "status":200,
  "time":"2023-03-23 21:38:40"
}
```
æ„Ÿè°¢ [@hiyouga]() å®ç°äº† OpenAI æ ¼å¼çš„æµå¼ API éƒ¨ç½²ï¼Œå¯ä»¥ä½œä¸ºä»»æ„åŸºäº ChatGPT çš„åº”ç”¨çš„åç«¯ï¼Œæ¯”å¦‚ [ChatGPT-Next-Web](https://github.com/Yidadaa/ChatGPT-Next-Web)ã€‚å¯ä»¥é€šè¿‡è¿è¡Œä»“åº“ä¸­çš„[openai_api.py](openai_api.py) è¿›è¡Œéƒ¨ç½²ï¼š
```shell
python openai_api.py
```
è¿›è¡Œ API è°ƒç”¨çš„ç¤ºä¾‹ä»£ç ä¸º
```python
import openai
if __name__ == "__main__":
    openai.api_base = "http://localhost:8000/v1"
    openai.api_key = "none"
    for chunk in openai.ChatCompletion.create(
        model="chatglm2-6b",
        messages=[
            {"role": "user", "content": "ä½ å¥½"}
        ],
        stream=True
    ):
        if hasattr(chunk.choices[0].delta, "content"):
            print(chunk.choices[0].delta.content, end="", flush=True)
```


## ä½æˆæœ¬éƒ¨ç½²

### æ¨¡å‹é‡åŒ–

é»˜è®¤æƒ…å†µä¸‹ï¼Œæ¨¡å‹ä»¥ FP16 ç²¾åº¦åŠ è½½ï¼Œè¿è¡Œä¸Šè¿°ä»£ç éœ€è¦å¤§æ¦‚ 13GB æ˜¾å­˜ã€‚å¦‚æœä½ çš„ GPU æ˜¾å­˜æœ‰é™ï¼Œå¯ä»¥å°è¯•ä»¥é‡åŒ–æ–¹å¼åŠ è½½æ¨¡å‹ï¼Œä½¿ç”¨æ–¹æ³•å¦‚ä¸‹ï¼š

```python
# æŒ‰éœ€ä¿®æ”¹ï¼Œç›®å‰åªæ”¯æŒ 4/8 bit é‡åŒ–
model = AutoModel.from_pretrained("THUDM/chatglm2-6b", trust_remote_code=True).quantize(8).cuda()
```

æ¨¡å‹é‡åŒ–ä¼šå¸¦æ¥ä¸€å®šçš„æ€§èƒ½æŸå¤±ï¼Œç»è¿‡æµ‹è¯•ï¼ŒChatGLM2-6B åœ¨ 4-bit é‡åŒ–ä¸‹ä»ç„¶èƒ½å¤Ÿè¿›è¡Œè‡ªç„¶æµç•…çš„ç”Ÿæˆã€‚

å¦‚æœä½ çš„å†…å­˜ä¸è¶³ï¼Œå¯ä»¥ç›´æ¥åŠ è½½é‡åŒ–åçš„æ¨¡å‹ï¼š
```python
model = AutoModel.from_pretrained("THUDM/chatglm2-6b-int4",trust_remote_code=True).cuda()
```

<!-- é‡åŒ–æ¨¡å‹çš„å‚æ•°æ–‡ä»¶ä¹Ÿå¯ä»¥ä»[è¿™é‡Œ](https://cloud.tsinghua.edu.cn/d/674208019e314311ab5c/)æ‰‹åŠ¨ä¸‹è½½ã€‚ -->

### CPU éƒ¨ç½²

å¦‚æœä½ æ²¡æœ‰ GPU ç¡¬ä»¶çš„è¯ï¼Œä¹Ÿå¯ä»¥åœ¨ CPU ä¸Šè¿›è¡Œæ¨ç†ï¼Œä½†æ˜¯æ¨ç†é€Ÿåº¦ä¼šæ›´æ…¢ã€‚ä½¿ç”¨æ–¹æ³•å¦‚ä¸‹ï¼ˆéœ€è¦å¤§æ¦‚ 32GB å†…å­˜ï¼‰
```python
model = AutoModel.from_pretrained("THUDM/chatglm2-6b", trust_remote_code=True).float()
```
å¦‚æœä½ çš„å†…å­˜ä¸è¶³çš„è¯ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨é‡åŒ–åçš„æ¨¡å‹
```python
model = AutoModel.from_pretrained("THUDM/chatglm2-6b-int4",trust_remote_code=True).float()
```
åœ¨ cpu ä¸Šè¿è¡Œé‡åŒ–åçš„æ¨¡å‹éœ€è¦å®‰è£… `gcc` ä¸ `openmp`ã€‚å¤šæ•° Linux å‘è¡Œç‰ˆé»˜è®¤å·²å®‰è£…ã€‚å¯¹äº Windows ï¼Œå¯åœ¨å®‰è£… [TDM-GCC](https://jmeubank.github.io/tdm-gcc/) æ—¶å‹¾é€‰ `openmp`ã€‚ Windows æµ‹è¯•ç¯å¢ƒ `gcc` ç‰ˆæœ¬ä¸º `TDM-GCC 10.3.0`ï¼Œ Linux ä¸º `gcc 11.3.0`ã€‚åœ¨ MacOS ä¸Šè¯·å‚è€ƒ [Q1](FAQ.md#q1)ã€‚

### Mac éƒ¨ç½²

å¯¹äºæ­è½½äº† Apple Silicon æˆ–è€… AMD GPU çš„ Macï¼Œå¯ä»¥ä½¿ç”¨ MPS åç«¯æ¥åœ¨ GPU ä¸Šè¿è¡Œ ChatGLM2-6Bã€‚éœ€è¦å‚è€ƒ Apple çš„ [å®˜æ–¹è¯´æ˜](https://developer.apple.com/metal/pytorch) å®‰è£… PyTorch-Nightlyï¼ˆæ­£ç¡®çš„ç‰ˆæœ¬å·åº”è¯¥æ˜¯2.x.x.dev2023xxxxï¼Œè€Œä¸æ˜¯ 2.x.xï¼‰ã€‚

ç›®å‰åœ¨ MacOS ä¸Šåªæ”¯æŒ[ä»æœ¬åœ°åŠ è½½æ¨¡å‹](README.md#ä»æœ¬åœ°åŠ è½½æ¨¡å‹)ã€‚å°†ä»£ç ä¸­çš„æ¨¡å‹åŠ è½½æ”¹ä¸ºä»æœ¬åœ°åŠ è½½ï¼Œå¹¶ä½¿ç”¨ mps åç«¯ï¼š
```python
model = AutoModel.from_pretrained("your local path", trust_remote_code=True).to('mps')
```

åŠ è½½åŠç²¾åº¦çš„ ChatGLM2-6B æ¨¡å‹éœ€è¦å¤§æ¦‚ 13GB å†…å­˜ã€‚å†…å­˜è¾ƒå°çš„æœºå™¨ï¼ˆæ¯”å¦‚ 16GB å†…å­˜çš„ MacBook Proï¼‰ï¼Œåœ¨ç©ºä½™å†…å­˜ä¸è¶³çš„æƒ…å†µä¸‹ä¼šä½¿ç”¨ç¡¬ç›˜ä¸Šçš„è™šæ‹Ÿå†…å­˜ï¼Œå¯¼è‡´æ¨ç†é€Ÿåº¦ä¸¥é‡å˜æ…¢ã€‚
æ­¤æ—¶å¯ä»¥ä½¿ç”¨é‡åŒ–åçš„æ¨¡å‹ chatglm2-6b-int4ã€‚å› ä¸º GPU ä¸Šé‡åŒ–çš„ kernel æ˜¯ä½¿ç”¨ CUDA ç¼–å†™çš„ï¼Œå› æ­¤æ— æ³•åœ¨ MacOS ä¸Šä½¿ç”¨ï¼Œåªèƒ½ä½¿ç”¨ CPU è¿›è¡Œæ¨ç†ã€‚
ä¸ºäº†å……åˆ†ä½¿ç”¨ CPU å¹¶è¡Œï¼Œè¿˜éœ€è¦[å•ç‹¬å®‰è£… OpenMP](FAQ.md#q1)ã€‚

### å¤šå¡éƒ¨ç½²
å¦‚æœä½ æœ‰å¤šå¼  GPUï¼Œä½†æ˜¯æ¯å¼  GPU çš„æ˜¾å­˜å¤§å°éƒ½ä¸è¶³ä»¥å®¹çº³å®Œæ•´çš„æ¨¡å‹ï¼Œé‚£ä¹ˆå¯ä»¥å°†æ¨¡å‹åˆ‡åˆ†åœ¨å¤šå¼ GPUä¸Šã€‚é¦–å…ˆå®‰è£… accelerate: `pip install accelerate`ï¼Œç„¶åé€šè¿‡å¦‚ä¸‹æ–¹æ³•åŠ è½½æ¨¡å‹ï¼š
```python
from utils import load_model_on_gpus
model = load_model_on_gpus("THUDM/chatglm2-6b", num_gpus=2)
```
å³å¯å°†æ¨¡å‹éƒ¨ç½²åˆ°ä¸¤å¼  GPU ä¸Šè¿›è¡Œæ¨ç†ã€‚ä½ å¯ä»¥å°† `num_gpus` æ”¹ä¸ºä½ å¸Œæœ›ä½¿ç”¨çš„ GPU æ•°ã€‚é»˜è®¤æ˜¯å‡åŒ€åˆ‡åˆ†çš„ï¼Œä½ ä¹Ÿå¯ä»¥ä¼ å…¥ `device_map` å‚æ•°æ¥è‡ªå·±æŒ‡å®šã€‚ 

## åè®®

æœ¬ä»“åº“çš„ä»£ç ä¾ç…§ [Apache-2.0](https://www.apache.org/licenses/LICENSE-2.0) åè®®å¼€æºï¼ŒChatGLM2-6B æ¨¡å‹çš„æƒé‡çš„ä½¿ç”¨åˆ™éœ€è¦éµå¾ª [Model License](MODEL_LICENSE)ã€‚ChatGLM2-6B æƒé‡å¯¹å­¦æœ¯ç ”ç©¶**å®Œå…¨å¼€æ”¾**ï¼Œåœ¨è·å¾—å®˜æ–¹çš„ä¹¦é¢è®¸å¯åï¼Œäº¦**å…è®¸å•†ä¸šä½¿ç”¨**ã€‚å¦‚æœæ‚¨å‘ç°æˆ‘ä»¬çš„å¼€æºæ¨¡å‹å¯¹æ‚¨çš„ä¸šåŠ¡æœ‰ç”¨ï¼Œæˆ‘ä»¬æ¬¢è¿æ‚¨å¯¹ä¸‹ä¸€ä»£æ¨¡å‹ ChatGLM3 ç ”å‘çš„æèµ ã€‚ç”³è¯·å•†ç”¨è®¸å¯ä¸æèµ è¯·è”ç³» [yiwen.xu@zhipuai.cn](mailto:yiwen.xu@zhipuai.cn)ã€‚ 


## å¼•ç”¨

å¦‚æœä½ è§‰å¾—æˆ‘ä»¬çš„å·¥ä½œæœ‰å¸®åŠ©çš„è¯ï¼Œè¯·è€ƒè™‘å¼•ç”¨ä¸‹åˆ—è®ºæ–‡ï¼ŒChatGLM2-6B çš„è®ºæ–‡ä¼šåœ¨è¿‘æœŸå…¬å¸ƒï¼Œæ•¬è¯·æœŸå¾…ï½

```
@article{zeng2022glm,
  title={Glm-130b: An open bilingual pre-trained model},
  author={Zeng, Aohan and Liu, Xiao and Du, Zhengxiao and Wang, Zihan and Lai, Hanyu and Ding, Ming and Yang, Zhuoyi and Xu, Yifan and Zheng, Wendi and Xia, Xiao and others},
  journal={arXiv preprint arXiv:2210.02414},
  year={2022}
}
```
```
@inproceedings{du2022glm,
  title={GLM: General Language Model Pretraining with Autoregressive Blank Infilling},
  author={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={320--335},
  year={2022}
}
```
